{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN-LSTM-keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5wE1c6TaCwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXJD5ZTKa0ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We have to create a model which predicts the next digit given the previous digit\n",
        "#for eg- if we give 4 5 6 7 8 9 10 the prediction should be 11 (we would be using length of 7)\n",
        "\n",
        "#Firstly lets prepare the data\n",
        "\n",
        "data = [[[(i+j)/100] for i in range(7)] for j in range(100)]\n",
        "target = [(i+7)/100 for i in range(100)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InxkCI1uci1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data= np.array(data, dtype=float)\n",
        "target = np.array(target, dtype=float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S2PfwZdcmGp",
        "colab_type": "code",
        "outputId": "289ba476-cbc2-44a3-ed12-1e53d206993e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 7, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqp5zmu-dFIQ",
        "colab_type": "code",
        "outputId": "d7d99292-b08f-476e-a767-228d170637b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "target.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ry9JsS1dL7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(data,target,test_size=0.2,random_state = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQjrAT0Sdl-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RNN will be implemented now\n",
        "\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VcpSv3IeZji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now here we will be adding the parameters of LSTM\n",
        "#The fist 1 parameter determines the output size \n",
        "#Batch input shape describes the input shape of the data\n",
        "#format no of inputs, length of sequence,length of each vector\n",
        "model.add(LSTM((1),batch_input_shape=(None,7,1),return_sequences= False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocuFBFbB3KmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='mean_absolute_error',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoHKKfgE4WJg",
        "colab_type": "code",
        "outputId": "411102b5-baa0-448d-f463-c66b35dd8185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (None, 1)                 12        \n",
            "=================================================================\n",
            "Total params: 12\n",
            "Trainable params: 12\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nowYG9S4YWH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "51868c0a-dcb9-4b9b-dd09-f428d7dab81a"
      },
      "source": [
        "history = model.fit(x_train,y_train,epochs=500,validation_data=(x_test,y_test))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 80 samples, validate on 20 samples\n",
            "Epoch 1/500\n",
            "80/80 [==============================] - 0s 410us/step - loss: 0.3077 - acc: 0.0000e+00 - val_loss: 0.2028 - val_acc: 0.0000e+00\n",
            "Epoch 2/500\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.3043 - acc: 0.0000e+00 - val_loss: 0.1999 - val_acc: 0.0000e+00\n",
            "Epoch 3/500\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.3012 - acc: 0.0000e+00 - val_loss: 0.1970 - val_acc: 0.0000e+00\n",
            "Epoch 4/500\n",
            "80/80 [==============================] - 0s 320us/step - loss: 0.2980 - acc: 0.0000e+00 - val_loss: 0.1941 - val_acc: 0.0000e+00\n",
            "Epoch 5/500\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.2948 - acc: 0.0000e+00 - val_loss: 0.1913 - val_acc: 0.0000e+00\n",
            "Epoch 6/500\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.2920 - acc: 0.0000e+00 - val_loss: 0.1885 - val_acc: 0.0000e+00\n",
            "Epoch 7/500\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.2896 - acc: 0.0000e+00 - val_loss: 0.1862 - val_acc: 0.0000e+00\n",
            "Epoch 8/500\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.2867 - acc: 0.0000e+00 - val_loss: 0.1841 - val_acc: 0.0000e+00\n",
            "Epoch 9/500\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.2838 - acc: 0.0000e+00 - val_loss: 0.1821 - val_acc: 0.0000e+00\n",
            "Epoch 10/500\n",
            "80/80 [==============================] - 0s 332us/step - loss: 0.2813 - acc: 0.0000e+00 - val_loss: 0.1801 - val_acc: 0.0000e+00\n",
            "Epoch 11/500\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.2792 - acc: 0.0000e+00 - val_loss: 0.1785 - val_acc: 0.0000e+00\n",
            "Epoch 12/500\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.2769 - acc: 0.0000e+00 - val_loss: 0.1771 - val_acc: 0.0000e+00\n",
            "Epoch 13/500\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.2742 - acc: 0.0000e+00 - val_loss: 0.1762 - val_acc: 0.0000e+00\n",
            "Epoch 14/500\n",
            "80/80 [==============================] - 0s 349us/step - loss: 0.2723 - acc: 0.0000e+00 - val_loss: 0.1752 - val_acc: 0.0000e+00\n",
            "Epoch 15/500\n",
            "80/80 [==============================] - 0s 339us/step - loss: 0.2701 - acc: 0.0000e+00 - val_loss: 0.1743 - val_acc: 0.0000e+00\n",
            "Epoch 16/500\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.2680 - acc: 0.0000e+00 - val_loss: 0.1734 - val_acc: 0.0000e+00\n",
            "Epoch 17/500\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.2662 - acc: 0.0000e+00 - val_loss: 0.1729 - val_acc: 0.0000e+00\n",
            "Epoch 18/500\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.2642 - acc: 0.0000e+00 - val_loss: 0.1725 - val_acc: 0.0000e+00\n",
            "Epoch 19/500\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.2622 - acc: 0.0000e+00 - val_loss: 0.1720 - val_acc: 0.0000e+00\n",
            "Epoch 20/500\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.2607 - acc: 0.0000e+00 - val_loss: 0.1715 - val_acc: 0.0000e+00\n",
            "Epoch 21/500\n",
            "80/80 [==============================] - 0s 243us/step - loss: 0.2588 - acc: 0.0000e+00 - val_loss: 0.1711 - val_acc: 0.0000e+00\n",
            "Epoch 22/500\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.2574 - acc: 0.0000e+00 - val_loss: 0.1706 - val_acc: 0.0000e+00\n",
            "Epoch 23/500\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.2559 - acc: 0.0000e+00 - val_loss: 0.1702 - val_acc: 0.0000e+00\n",
            "Epoch 24/500\n",
            "80/80 [==============================] - 0s 231us/step - loss: 0.2545 - acc: 0.0125 - val_loss: 0.1700 - val_acc: 0.0000e+00\n",
            "Epoch 25/500\n",
            "80/80 [==============================] - 0s 236us/step - loss: 0.2529 - acc: 0.0125 - val_loss: 0.1699 - val_acc: 0.0000e+00\n",
            "Epoch 26/500\n",
            "80/80 [==============================] - 0s 229us/step - loss: 0.2518 - acc: 0.0125 - val_loss: 0.1698 - val_acc: 0.0000e+00\n",
            "Epoch 27/500\n",
            "80/80 [==============================] - 0s 269us/step - loss: 0.2501 - acc: 0.0125 - val_loss: 0.1696 - val_acc: 0.0000e+00\n",
            "Epoch 28/500\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.2488 - acc: 0.0125 - val_loss: 0.1694 - val_acc: 0.0000e+00\n",
            "Epoch 29/500\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.2477 - acc: 0.0125 - val_loss: 0.1693 - val_acc: 0.0000e+00\n",
            "Epoch 30/500\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.2465 - acc: 0.0125 - val_loss: 0.1694 - val_acc: 0.0000e+00\n",
            "Epoch 31/500\n",
            "80/80 [==============================] - 0s 334us/step - loss: 0.2453 - acc: 0.0125 - val_loss: 0.1694 - val_acc: 0.0000e+00\n",
            "Epoch 32/500\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.2440 - acc: 0.0125 - val_loss: 0.1695 - val_acc: 0.0000e+00\n",
            "Epoch 33/500\n",
            "80/80 [==============================] - 0s 251us/step - loss: 0.2428 - acc: 0.0125 - val_loss: 0.1695 - val_acc: 0.0000e+00\n",
            "Epoch 34/500\n",
            "80/80 [==============================] - 0s 253us/step - loss: 0.2416 - acc: 0.0125 - val_loss: 0.1695 - val_acc: 0.0000e+00\n",
            "Epoch 35/500\n",
            "80/80 [==============================] - 0s 258us/step - loss: 0.2405 - acc: 0.0125 - val_loss: 0.1695 - val_acc: 0.0000e+00\n",
            "Epoch 36/500\n",
            "80/80 [==============================] - 0s 260us/step - loss: 0.2396 - acc: 0.0125 - val_loss: 0.1695 - val_acc: 0.0000e+00\n",
            "Epoch 37/500\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.2383 - acc: 0.0125 - val_loss: 0.1693 - val_acc: 0.0000e+00\n",
            "Epoch 38/500\n",
            "80/80 [==============================] - 0s 241us/step - loss: 0.2373 - acc: 0.0125 - val_loss: 0.1691 - val_acc: 0.0000e+00\n",
            "Epoch 39/500\n",
            "80/80 [==============================] - 0s 236us/step - loss: 0.2363 - acc: 0.0125 - val_loss: 0.1690 - val_acc: 0.0000e+00\n",
            "Epoch 40/500\n",
            "80/80 [==============================] - 0s 246us/step - loss: 0.2353 - acc: 0.0125 - val_loss: 0.1688 - val_acc: 0.0000e+00\n",
            "Epoch 41/500\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.2343 - acc: 0.0125 - val_loss: 0.1686 - val_acc: 0.0000e+00\n",
            "Epoch 42/500\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.2333 - acc: 0.0125 - val_loss: 0.1684 - val_acc: 0.0000e+00\n",
            "Epoch 43/500\n",
            "80/80 [==============================] - 0s 257us/step - loss: 0.2323 - acc: 0.0125 - val_loss: 0.1682 - val_acc: 0.0000e+00\n",
            "Epoch 44/500\n",
            "80/80 [==============================] - 0s 230us/step - loss: 0.2314 - acc: 0.0125 - val_loss: 0.1679 - val_acc: 0.0000e+00\n",
            "Epoch 45/500\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.2305 - acc: 0.0125 - val_loss: 0.1677 - val_acc: 0.0000e+00\n",
            "Epoch 46/500\n",
            "80/80 [==============================] - 0s 234us/step - loss: 0.2294 - acc: 0.0125 - val_loss: 0.1674 - val_acc: 0.0000e+00\n",
            "Epoch 47/500\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.2284 - acc: 0.0125 - val_loss: 0.1672 - val_acc: 0.0000e+00\n",
            "Epoch 48/500\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.2275 - acc: 0.0125 - val_loss: 0.1670 - val_acc: 0.0000e+00\n",
            "Epoch 49/500\n",
            "80/80 [==============================] - 0s 252us/step - loss: 0.2265 - acc: 0.0125 - val_loss: 0.1668 - val_acc: 0.0000e+00\n",
            "Epoch 50/500\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.2256 - acc: 0.0125 - val_loss: 0.1667 - val_acc: 0.0000e+00\n",
            "Epoch 51/500\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.2246 - acc: 0.0125 - val_loss: 0.1666 - val_acc: 0.0000e+00\n",
            "Epoch 52/500\n",
            "80/80 [==============================] - 0s 254us/step - loss: 0.2237 - acc: 0.0125 - val_loss: 0.1664 - val_acc: 0.0000e+00\n",
            "Epoch 53/500\n",
            "80/80 [==============================] - 0s 240us/step - loss: 0.2226 - acc: 0.0125 - val_loss: 0.1661 - val_acc: 0.0000e+00\n",
            "Epoch 54/500\n",
            "80/80 [==============================] - 0s 253us/step - loss: 0.2217 - acc: 0.0125 - val_loss: 0.1658 - val_acc: 0.0000e+00\n",
            "Epoch 55/500\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.2208 - acc: 0.0125 - val_loss: 0.1654 - val_acc: 0.0000e+00\n",
            "Epoch 56/500\n",
            "80/80 [==============================] - 0s 252us/step - loss: 0.2199 - acc: 0.0125 - val_loss: 0.1649 - val_acc: 0.0000e+00\n",
            "Epoch 57/500\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.2189 - acc: 0.0125 - val_loss: 0.1644 - val_acc: 0.0000e+00\n",
            "Epoch 58/500\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.2180 - acc: 0.0125 - val_loss: 0.1640 - val_acc: 0.0000e+00\n",
            "Epoch 59/500\n",
            "80/80 [==============================] - 0s 251us/step - loss: 0.2170 - acc: 0.0125 - val_loss: 0.1636 - val_acc: 0.0000e+00\n",
            "Epoch 60/500\n",
            "80/80 [==============================] - 0s 233us/step - loss: 0.2161 - acc: 0.0125 - val_loss: 0.1633 - val_acc: 0.0000e+00\n",
            "Epoch 61/500\n",
            "80/80 [==============================] - 0s 271us/step - loss: 0.2152 - acc: 0.0125 - val_loss: 0.1631 - val_acc: 0.0000e+00\n",
            "Epoch 62/500\n",
            "80/80 [==============================] - 0s 258us/step - loss: 0.2143 - acc: 0.0125 - val_loss: 0.1627 - val_acc: 0.0000e+00\n",
            "Epoch 63/500\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.2133 - acc: 0.0125 - val_loss: 0.1622 - val_acc: 0.0000e+00\n",
            "Epoch 64/500\n",
            "80/80 [==============================] - 0s 334us/step - loss: 0.2124 - acc: 0.0125 - val_loss: 0.1617 - val_acc: 0.0000e+00\n",
            "Epoch 65/500\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.2115 - acc: 0.0125 - val_loss: 0.1612 - val_acc: 0.0000e+00\n",
            "Epoch 66/500\n",
            "80/80 [==============================] - 0s 265us/step - loss: 0.2105 - acc: 0.0125 - val_loss: 0.1606 - val_acc: 0.0000e+00\n",
            "Epoch 67/500\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.2096 - acc: 0.0125 - val_loss: 0.1600 - val_acc: 0.0000e+00\n",
            "Epoch 68/500\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.2086 - acc: 0.0125 - val_loss: 0.1593 - val_acc: 0.0000e+00\n",
            "Epoch 69/500\n",
            "80/80 [==============================] - 0s 267us/step - loss: 0.2077 - acc: 0.0125 - val_loss: 0.1587 - val_acc: 0.0000e+00\n",
            "Epoch 70/500\n",
            "80/80 [==============================] - 0s 330us/step - loss: 0.2068 - acc: 0.0125 - val_loss: 0.1581 - val_acc: 0.0000e+00\n",
            "Epoch 71/500\n",
            "80/80 [==============================] - 0s 253us/step - loss: 0.2059 - acc: 0.0125 - val_loss: 0.1577 - val_acc: 0.0000e+00\n",
            "Epoch 72/500\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.2049 - acc: 0.0125 - val_loss: 0.1571 - val_acc: 0.0000e+00\n",
            "Epoch 73/500\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.2039 - acc: 0.0125 - val_loss: 0.1566 - val_acc: 0.0000e+00\n",
            "Epoch 74/500\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.2029 - acc: 0.0125 - val_loss: 0.1560 - val_acc: 0.0000e+00\n",
            "Epoch 75/500\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.2020 - acc: 0.0125 - val_loss: 0.1554 - val_acc: 0.0000e+00\n",
            "Epoch 76/500\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.2010 - acc: 0.0125 - val_loss: 0.1548 - val_acc: 0.0000e+00\n",
            "Epoch 77/500\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.2001 - acc: 0.0125 - val_loss: 0.1540 - val_acc: 0.0000e+00\n",
            "Epoch 78/500\n",
            "80/80 [==============================] - 0s 404us/step - loss: 0.1992 - acc: 0.0125 - val_loss: 0.1534 - val_acc: 0.0000e+00\n",
            "Epoch 79/500\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.1982 - acc: 0.0125 - val_loss: 0.1527 - val_acc: 0.0000e+00\n",
            "Epoch 80/500\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.1973 - acc: 0.0125 - val_loss: 0.1523 - val_acc: 0.0000e+00\n",
            "Epoch 81/500\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.1963 - acc: 0.0125 - val_loss: 0.1517 - val_acc: 0.0000e+00\n",
            "Epoch 82/500\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.1954 - acc: 0.0125 - val_loss: 0.1512 - val_acc: 0.0000e+00\n",
            "Epoch 83/500\n",
            "80/80 [==============================] - 0s 441us/step - loss: 0.1944 - acc: 0.0125 - val_loss: 0.1506 - val_acc: 0.0000e+00\n",
            "Epoch 84/500\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.1934 - acc: 0.0125 - val_loss: 0.1500 - val_acc: 0.0000e+00\n",
            "Epoch 85/500\n",
            "80/80 [==============================] - 0s 256us/step - loss: 0.1925 - acc: 0.0125 - val_loss: 0.1493 - val_acc: 0.0000e+00\n",
            "Epoch 86/500\n",
            "80/80 [==============================] - 0s 231us/step - loss: 0.1916 - acc: 0.0125 - val_loss: 0.1487 - val_acc: 0.0000e+00\n",
            "Epoch 87/500\n",
            "80/80 [==============================] - 0s 238us/step - loss: 0.1906 - acc: 0.0125 - val_loss: 0.1480 - val_acc: 0.0000e+00\n",
            "Epoch 88/500\n",
            "80/80 [==============================] - 0s 249us/step - loss: 0.1896 - acc: 0.0125 - val_loss: 0.1474 - val_acc: 0.0000e+00\n",
            "Epoch 89/500\n",
            "80/80 [==============================] - 0s 255us/step - loss: 0.1886 - acc: 0.0125 - val_loss: 0.1467 - val_acc: 0.0000e+00\n",
            "Epoch 90/500\n",
            "80/80 [==============================] - 0s 245us/step - loss: 0.1876 - acc: 0.0125 - val_loss: 0.1460 - val_acc: 0.0000e+00\n",
            "Epoch 91/500\n",
            "80/80 [==============================] - 0s 258us/step - loss: 0.1867 - acc: 0.0125 - val_loss: 0.1454 - val_acc: 0.0000e+00\n",
            "Epoch 92/500\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.1858 - acc: 0.0125 - val_loss: 0.1448 - val_acc: 0.0000e+00\n",
            "Epoch 93/500\n",
            "80/80 [==============================] - 0s 253us/step - loss: 0.1848 - acc: 0.0125 - val_loss: 0.1441 - val_acc: 0.0000e+00\n",
            "Epoch 94/500\n",
            "80/80 [==============================] - 0s 245us/step - loss: 0.1839 - acc: 0.0125 - val_loss: 0.1432 - val_acc: 0.0000e+00\n",
            "Epoch 95/500\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.1829 - acc: 0.0125 - val_loss: 0.1425 - val_acc: 0.0000e+00\n",
            "Epoch 96/500\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.1819 - acc: 0.0125 - val_loss: 0.1419 - val_acc: 0.0000e+00\n",
            "Epoch 97/500\n",
            "80/80 [==============================] - 0s 342us/step - loss: 0.1809 - acc: 0.0125 - val_loss: 0.1413 - val_acc: 0.0000e+00\n",
            "Epoch 98/500\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.1799 - acc: 0.0125 - val_loss: 0.1407 - val_acc: 0.0000e+00\n",
            "Epoch 99/500\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.1790 - acc: 0.0125 - val_loss: 0.1402 - val_acc: 0.0000e+00\n",
            "Epoch 100/500\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.1780 - acc: 0.0125 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "Epoch 101/500\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.1771 - acc: 0.0125 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 102/500\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.1761 - acc: 0.0125 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 103/500\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.1751 - acc: 0.0125 - val_loss: 0.1377 - val_acc: 0.0000e+00\n",
            "Epoch 104/500\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.1741 - acc: 0.0125 - val_loss: 0.1368 - val_acc: 0.0000e+00\n",
            "Epoch 105/500\n",
            "80/80 [==============================] - 0s 327us/step - loss: 0.1731 - acc: 0.0125 - val_loss: 0.1359 - val_acc: 0.0000e+00\n",
            "Epoch 106/500\n",
            "80/80 [==============================] - 0s 269us/step - loss: 0.1721 - acc: 0.0125 - val_loss: 0.1351 - val_acc: 0.0000e+00\n",
            "Epoch 107/500\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.1711 - acc: 0.0125 - val_loss: 0.1344 - val_acc: 0.0000e+00\n",
            "Epoch 108/500\n",
            "80/80 [==============================] - 0s 350us/step - loss: 0.1702 - acc: 0.0125 - val_loss: 0.1336 - val_acc: 0.0000e+00\n",
            "Epoch 109/500\n",
            "80/80 [==============================] - 0s 349us/step - loss: 0.1692 - acc: 0.0125 - val_loss: 0.1329 - val_acc: 0.0000e+00\n",
            "Epoch 110/500\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.1682 - acc: 0.0125 - val_loss: 0.1322 - val_acc: 0.0000e+00\n",
            "Epoch 111/500\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.1672 - acc: 0.0125 - val_loss: 0.1315 - val_acc: 0.0000e+00\n",
            "Epoch 112/500\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.1662 - acc: 0.0125 - val_loss: 0.1308 - val_acc: 0.0000e+00\n",
            "Epoch 113/500\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.1652 - acc: 0.0125 - val_loss: 0.1300 - val_acc: 0.0000e+00\n",
            "Epoch 114/500\n",
            "80/80 [==============================] - 0s 269us/step - loss: 0.1642 - acc: 0.0125 - val_loss: 0.1292 - val_acc: 0.0000e+00\n",
            "Epoch 115/500\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.1633 - acc: 0.0125 - val_loss: 0.1284 - val_acc: 0.0000e+00\n",
            "Epoch 116/500\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.1622 - acc: 0.0125 - val_loss: 0.1275 - val_acc: 0.0000e+00\n",
            "Epoch 117/500\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.1612 - acc: 0.0125 - val_loss: 0.1267 - val_acc: 0.0000e+00\n",
            "Epoch 118/500\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.1602 - acc: 0.0125 - val_loss: 0.1258 - val_acc: 0.0000e+00\n",
            "Epoch 119/500\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.1592 - acc: 0.0125 - val_loss: 0.1250 - val_acc: 0.0000e+00\n",
            "Epoch 120/500\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.1582 - acc: 0.0125 - val_loss: 0.1241 - val_acc: 0.0000e+00\n",
            "Epoch 121/500\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.1572 - acc: 0.0125 - val_loss: 0.1234 - val_acc: 0.0000e+00\n",
            "Epoch 122/500\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.1562 - acc: 0.0125 - val_loss: 0.1226 - val_acc: 0.0000e+00\n",
            "Epoch 123/500\n",
            "80/80 [==============================] - 0s 268us/step - loss: 0.1552 - acc: 0.0125 - val_loss: 0.1217 - val_acc: 0.0000e+00\n",
            "Epoch 124/500\n",
            "80/80 [==============================] - 0s 416us/step - loss: 0.1542 - acc: 0.0125 - val_loss: 0.1208 - val_acc: 0.0000e+00\n",
            "Epoch 125/500\n",
            "80/80 [==============================] - 0s 254us/step - loss: 0.1532 - acc: 0.0125 - val_loss: 0.1200 - val_acc: 0.0000e+00\n",
            "Epoch 126/500\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.1522 - acc: 0.0125 - val_loss: 0.1194 - val_acc: 0.0000e+00\n",
            "Epoch 127/500\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.1512 - acc: 0.0125 - val_loss: 0.1188 - val_acc: 0.0000e+00\n",
            "Epoch 128/500\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.1501 - acc: 0.0125 - val_loss: 0.1182 - val_acc: 0.0000e+00\n",
            "Epoch 129/500\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.1491 - acc: 0.0125 - val_loss: 0.1176 - val_acc: 0.0000e+00\n",
            "Epoch 130/500\n",
            "80/80 [==============================] - 0s 437us/step - loss: 0.1481 - acc: 0.0125 - val_loss: 0.1170 - val_acc: 0.0000e+00\n",
            "Epoch 131/500\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.1470 - acc: 0.0125 - val_loss: 0.1165 - val_acc: 0.0000e+00\n",
            "Epoch 132/500\n",
            "80/80 [==============================] - 0s 250us/step - loss: 0.1460 - acc: 0.0125 - val_loss: 0.1160 - val_acc: 0.0000e+00\n",
            "Epoch 133/500\n",
            "80/80 [==============================] - 0s 249us/step - loss: 0.1450 - acc: 0.0125 - val_loss: 0.1155 - val_acc: 0.0000e+00\n",
            "Epoch 134/500\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.1440 - acc: 0.0125 - val_loss: 0.1148 - val_acc: 0.0000e+00\n",
            "Epoch 135/500\n",
            "80/80 [==============================] - 0s 267us/step - loss: 0.1430 - acc: 0.0125 - val_loss: 0.1142 - val_acc: 0.0000e+00\n",
            "Epoch 136/500\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.1419 - acc: 0.0125 - val_loss: 0.1134 - val_acc: 0.0000e+00\n",
            "Epoch 137/500\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.1408 - acc: 0.0125 - val_loss: 0.1124 - val_acc: 0.0000e+00\n",
            "Epoch 138/500\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.1398 - acc: 0.0125 - val_loss: 0.1114 - val_acc: 0.0000e+00\n",
            "Epoch 139/500\n",
            "80/80 [==============================] - 0s 403us/step - loss: 0.1387 - acc: 0.0125 - val_loss: 0.1103 - val_acc: 0.0000e+00\n",
            "Epoch 140/500\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.1377 - acc: 0.0125 - val_loss: 0.1093 - val_acc: 0.0000e+00\n",
            "Epoch 141/500\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.1366 - acc: 0.0125 - val_loss: 0.1083 - val_acc: 0.0000e+00\n",
            "Epoch 142/500\n",
            "80/80 [==============================] - 0s 347us/step - loss: 0.1356 - acc: 0.0125 - val_loss: 0.1073 - val_acc: 0.0000e+00\n",
            "Epoch 143/500\n",
            "80/80 [==============================] - 0s 343us/step - loss: 0.1345 - acc: 0.0125 - val_loss: 0.1064 - val_acc: 0.0000e+00\n",
            "Epoch 144/500\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.1336 - acc: 0.0125 - val_loss: 0.1056 - val_acc: 0.0000e+00\n",
            "Epoch 145/500\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.1324 - acc: 0.0125 - val_loss: 0.1047 - val_acc: 0.0000e+00\n",
            "Epoch 146/500\n",
            "80/80 [==============================] - 0s 335us/step - loss: 0.1314 - acc: 0.0125 - val_loss: 0.1037 - val_acc: 0.0000e+00\n",
            "Epoch 147/500\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.1303 - acc: 0.0125 - val_loss: 0.1027 - val_acc: 0.0000e+00\n",
            "Epoch 148/500\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.1292 - acc: 0.0125 - val_loss: 0.1020 - val_acc: 0.0000e+00\n",
            "Epoch 149/500\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.1282 - acc: 0.0125 - val_loss: 0.1011 - val_acc: 0.0000e+00\n",
            "Epoch 150/500\n",
            "80/80 [==============================] - 0s 320us/step - loss: 0.1271 - acc: 0.0125 - val_loss: 0.1003 - val_acc: 0.0000e+00\n",
            "Epoch 151/500\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.1261 - acc: 0.0125 - val_loss: 0.0995 - val_acc: 0.0000e+00\n",
            "Epoch 152/500\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.1250 - acc: 0.0125 - val_loss: 0.0986 - val_acc: 0.0000e+00\n",
            "Epoch 153/500\n",
            "80/80 [==============================] - 0s 353us/step - loss: 0.1239 - acc: 0.0125 - val_loss: 0.0976 - val_acc: 0.0000e+00\n",
            "Epoch 154/500\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.1228 - acc: 0.0125 - val_loss: 0.0968 - val_acc: 0.0000e+00\n",
            "Epoch 155/500\n",
            "80/80 [==============================] - 0s 314us/step - loss: 0.1218 - acc: 0.0125 - val_loss: 0.0958 - val_acc: 0.0000e+00\n",
            "Epoch 156/500\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.1207 - acc: 0.0125 - val_loss: 0.0951 - val_acc: 0.0000e+00\n",
            "Epoch 157/500\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.1196 - acc: 0.0125 - val_loss: 0.0944 - val_acc: 0.0000e+00\n",
            "Epoch 158/500\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.1185 - acc: 0.0125 - val_loss: 0.0936 - val_acc: 0.0000e+00\n",
            "Epoch 159/500\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.1174 - acc: 0.0125 - val_loss: 0.0928 - val_acc: 0.0000e+00\n",
            "Epoch 160/500\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.1164 - acc: 0.0125 - val_loss: 0.0918 - val_acc: 0.0000e+00\n",
            "Epoch 161/500\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.1153 - acc: 0.0125 - val_loss: 0.0909 - val_acc: 0.0000e+00\n",
            "Epoch 162/500\n",
            "80/80 [==============================] - 0s 350us/step - loss: 0.1142 - acc: 0.0125 - val_loss: 0.0900 - val_acc: 0.0000e+00\n",
            "Epoch 163/500\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.1131 - acc: 0.0125 - val_loss: 0.0891 - val_acc: 0.0000e+00\n",
            "Epoch 164/500\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.1120 - acc: 0.0125 - val_loss: 0.0882 - val_acc: 0.0000e+00\n",
            "Epoch 165/500\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.1109 - acc: 0.0125 - val_loss: 0.0873 - val_acc: 0.0000e+00\n",
            "Epoch 166/500\n",
            "80/80 [==============================] - 0s 326us/step - loss: 0.1098 - acc: 0.0125 - val_loss: 0.0865 - val_acc: 0.0000e+00\n",
            "Epoch 167/500\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.1087 - acc: 0.0125 - val_loss: 0.0857 - val_acc: 0.0000e+00\n",
            "Epoch 168/500\n",
            "80/80 [==============================] - 0s 349us/step - loss: 0.1076 - acc: 0.0125 - val_loss: 0.0849 - val_acc: 0.0000e+00\n",
            "Epoch 169/500\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.1065 - acc: 0.0125 - val_loss: 0.0839 - val_acc: 0.0000e+00\n",
            "Epoch 170/500\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.1054 - acc: 0.0125 - val_loss: 0.0831 - val_acc: 0.0000e+00\n",
            "Epoch 171/500\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.1042 - acc: 0.0125 - val_loss: 0.0822 - val_acc: 0.0000e+00\n",
            "Epoch 172/500\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.1031 - acc: 0.0125 - val_loss: 0.0813 - val_acc: 0.0000e+00\n",
            "Epoch 173/500\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.1020 - acc: 0.0125 - val_loss: 0.0805 - val_acc: 0.0000e+00\n",
            "Epoch 174/500\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.1009 - acc: 0.0125 - val_loss: 0.0797 - val_acc: 0.0000e+00\n",
            "Epoch 175/500\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.0997 - acc: 0.0125 - val_loss: 0.0787 - val_acc: 0.0000e+00\n",
            "Epoch 176/500\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.0986 - acc: 0.0125 - val_loss: 0.0778 - val_acc: 0.0000e+00\n",
            "Epoch 177/500\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.0974 - acc: 0.0125 - val_loss: 0.0768 - val_acc: 0.0000e+00\n",
            "Epoch 178/500\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.0963 - acc: 0.0125 - val_loss: 0.0758 - val_acc: 0.0000e+00\n",
            "Epoch 179/500\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.0951 - acc: 0.0125 - val_loss: 0.0748 - val_acc: 0.0000e+00\n",
            "Epoch 180/500\n",
            "80/80 [==============================] - 0s 246us/step - loss: 0.0939 - acc: 0.0125 - val_loss: 0.0739 - val_acc: 0.0000e+00\n",
            "Epoch 181/500\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.0927 - acc: 0.0125 - val_loss: 0.0731 - val_acc: 0.0000e+00\n",
            "Epoch 182/500\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.0916 - acc: 0.0125 - val_loss: 0.0722 - val_acc: 0.0000e+00\n",
            "Epoch 183/500\n",
            "80/80 [==============================] - 0s 269us/step - loss: 0.0904 - acc: 0.0125 - val_loss: 0.0713 - val_acc: 0.0000e+00\n",
            "Epoch 184/500\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.0893 - acc: 0.0125 - val_loss: 0.0703 - val_acc: 0.0000e+00\n",
            "Epoch 185/500\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.0881 - acc: 0.0125 - val_loss: 0.0693 - val_acc: 0.0000e+00\n",
            "Epoch 186/500\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.0869 - acc: 0.0125 - val_loss: 0.0683 - val_acc: 0.0000e+00\n",
            "Epoch 187/500\n",
            "80/80 [==============================] - 0s 256us/step - loss: 0.0857 - acc: 0.0125 - val_loss: 0.0671 - val_acc: 0.0000e+00\n",
            "Epoch 188/500\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.0846 - acc: 0.0125 - val_loss: 0.0659 - val_acc: 0.0000e+00\n",
            "Epoch 189/500\n",
            "80/80 [==============================] - 0s 264us/step - loss: 0.0833 - acc: 0.0125 - val_loss: 0.0648 - val_acc: 0.0000e+00\n",
            "Epoch 190/500\n",
            "80/80 [==============================] - 0s 256us/step - loss: 0.0821 - acc: 0.0125 - val_loss: 0.0637 - val_acc: 0.0000e+00\n",
            "Epoch 191/500\n",
            "80/80 [==============================] - 0s 258us/step - loss: 0.0809 - acc: 0.0125 - val_loss: 0.0628 - val_acc: 0.0000e+00\n",
            "Epoch 192/500\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.0797 - acc: 0.0125 - val_loss: 0.0620 - val_acc: 0.0000e+00\n",
            "Epoch 193/500\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.0785 - acc: 0.0125 - val_loss: 0.0611 - val_acc: 0.0000e+00\n",
            "Epoch 194/500\n",
            "80/80 [==============================] - 0s 249us/step - loss: 0.0774 - acc: 0.0125 - val_loss: 0.0602 - val_acc: 0.0000e+00\n",
            "Epoch 195/500\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.0761 - acc: 0.0125 - val_loss: 0.0591 - val_acc: 0.0000e+00\n",
            "Epoch 196/500\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.0749 - acc: 0.0125 - val_loss: 0.0579 - val_acc: 0.0000e+00\n",
            "Epoch 197/500\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.0736 - acc: 0.0125 - val_loss: 0.0567 - val_acc: 0.0000e+00\n",
            "Epoch 198/500\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.0724 - acc: 0.0125 - val_loss: 0.0555 - val_acc: 0.0000e+00\n",
            "Epoch 199/500\n",
            "80/80 [==============================] - 0s 261us/step - loss: 0.0712 - acc: 0.0125 - val_loss: 0.0544 - val_acc: 0.0000e+00\n",
            "Epoch 200/500\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.0700 - acc: 0.0125 - val_loss: 0.0535 - val_acc: 0.0000e+00\n",
            "Epoch 201/500\n",
            "80/80 [==============================] - 0s 261us/step - loss: 0.0688 - acc: 0.0125 - val_loss: 0.0525 - val_acc: 0.0000e+00\n",
            "Epoch 202/500\n",
            "80/80 [==============================] - 0s 271us/step - loss: 0.0675 - acc: 0.0125 - val_loss: 0.0514 - val_acc: 0.0000e+00\n",
            "Epoch 203/500\n",
            "80/80 [==============================] - 0s 347us/step - loss: 0.0662 - acc: 0.0125 - val_loss: 0.0504 - val_acc: 0.0000e+00\n",
            "Epoch 204/500\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.0650 - acc: 0.0125 - val_loss: 0.0493 - val_acc: 0.0000e+00\n",
            "Epoch 205/500\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.0637 - acc: 0.0125 - val_loss: 0.0484 - val_acc: 0.0000e+00\n",
            "Epoch 206/500\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.0624 - acc: 0.0125 - val_loss: 0.0472 - val_acc: 0.0000e+00\n",
            "Epoch 207/500\n",
            "80/80 [==============================] - 0s 334us/step - loss: 0.0611 - acc: 0.0125 - val_loss: 0.0460 - val_acc: 0.0000e+00\n",
            "Epoch 208/500\n",
            "80/80 [==============================] - 0s 343us/step - loss: 0.0599 - acc: 0.0125 - val_loss: 0.0447 - val_acc: 0.0000e+00\n",
            "Epoch 209/500\n",
            "80/80 [==============================] - 0s 256us/step - loss: 0.0586 - acc: 0.0125 - val_loss: 0.0436 - val_acc: 0.0000e+00\n",
            "Epoch 210/500\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.0572 - acc: 0.0125 - val_loss: 0.0425 - val_acc: 0.0000e+00\n",
            "Epoch 211/500\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.0560 - acc: 0.0125 - val_loss: 0.0416 - val_acc: 0.0000e+00\n",
            "Epoch 212/500\n",
            "80/80 [==============================] - 0s 313us/step - loss: 0.0547 - acc: 0.0125 - val_loss: 0.0405 - val_acc: 0.0000e+00\n",
            "Epoch 213/500\n",
            "80/80 [==============================] - 0s 259us/step - loss: 0.0533 - acc: 0.0125 - val_loss: 0.0392 - val_acc: 0.0000e+00\n",
            "Epoch 214/500\n",
            "80/80 [==============================] - 0s 356us/step - loss: 0.0520 - acc: 0.0125 - val_loss: 0.0380 - val_acc: 0.0000e+00\n",
            "Epoch 215/500\n",
            "80/80 [==============================] - 0s 333us/step - loss: 0.0508 - acc: 0.0125 - val_loss: 0.0369 - val_acc: 0.0000e+00\n",
            "Epoch 216/500\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.0494 - acc: 0.0125 - val_loss: 0.0358 - val_acc: 0.0000e+00\n",
            "Epoch 217/500\n",
            "80/80 [==============================] - 0s 331us/step - loss: 0.0481 - acc: 0.0125 - val_loss: 0.0347 - val_acc: 0.0000e+00\n",
            "Epoch 218/500\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.0467 - acc: 0.0125 - val_loss: 0.0336 - val_acc: 0.0000e+00\n",
            "Epoch 219/500\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.0455 - acc: 0.0125 - val_loss: 0.0325 - val_acc: 0.0000e+00\n",
            "Epoch 220/500\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.0443 - acc: 0.0125 - val_loss: 0.0317 - val_acc: 0.0000e+00\n",
            "Epoch 221/500\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.0430 - acc: 0.0125 - val_loss: 0.0305 - val_acc: 0.0000e+00\n",
            "Epoch 222/500\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.0418 - acc: 0.0125 - val_loss: 0.0294 - val_acc: 0.0000e+00\n",
            "Epoch 223/500\n",
            "80/80 [==============================] - 0s 489us/step - loss: 0.0407 - acc: 0.0125 - val_loss: 0.0284 - val_acc: 0.0000e+00\n",
            "Epoch 224/500\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.0396 - acc: 0.0125 - val_loss: 0.0275 - val_acc: 0.0000e+00\n",
            "Epoch 225/500\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.0386 - acc: 0.0125 - val_loss: 0.0263 - val_acc: 0.0000e+00\n",
            "Epoch 226/500\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.0376 - acc: 0.0125 - val_loss: 0.0254 - val_acc: 0.0000e+00\n",
            "Epoch 227/500\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.0366 - acc: 0.0125 - val_loss: 0.0244 - val_acc: 0.0000e+00\n",
            "Epoch 228/500\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.0358 - acc: 0.0125 - val_loss: 0.0238 - val_acc: 0.0000e+00\n",
            "Epoch 229/500\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.0351 - acc: 0.0125 - val_loss: 0.0234 - val_acc: 0.0000e+00\n",
            "Epoch 230/500\n",
            "80/80 [==============================] - 0s 352us/step - loss: 0.0343 - acc: 0.0125 - val_loss: 0.0218 - val_acc: 0.0000e+00\n",
            "Epoch 231/500\n",
            "80/80 [==============================] - 0s 267us/step - loss: 0.0340 - acc: 0.0125 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
            "Epoch 232/500\n",
            "80/80 [==============================] - 0s 267us/step - loss: 0.0332 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
            "Epoch 233/500\n",
            "80/80 [==============================] - 0s 315us/step - loss: 0.0332 - acc: 0.0125 - val_loss: 0.0216 - val_acc: 0.0000e+00\n",
            "Epoch 234/500\n",
            "80/80 [==============================] - 0s 351us/step - loss: 0.0324 - acc: 0.0125 - val_loss: 0.0201 - val_acc: 0.0000e+00\n",
            "Epoch 235/500\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.0322 - acc: 0.0125 - val_loss: 0.0196 - val_acc: 0.0000e+00\n",
            "Epoch 236/500\n",
            "80/80 [==============================] - 0s 261us/step - loss: 0.0317 - acc: 0.0125 - val_loss: 0.0198 - val_acc: 0.0000e+00\n",
            "Epoch 237/500\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.0313 - acc: 0.0125 - val_loss: 0.0202 - val_acc: 0.0000e+00\n",
            "Epoch 238/500\n",
            "80/80 [==============================] - 0s 268us/step - loss: 0.0311 - acc: 0.0125 - val_loss: 0.0197 - val_acc: 0.0000e+00\n",
            "Epoch 239/500\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.0307 - acc: 0.0125 - val_loss: 0.0191 - val_acc: 0.0000e+00\n",
            "Epoch 240/500\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.0305 - acc: 0.0125 - val_loss: 0.0187 - val_acc: 0.0000e+00\n",
            "Epoch 241/500\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.0302 - acc: 0.0125 - val_loss: 0.0186 - val_acc: 0.0000e+00\n",
            "Epoch 242/500\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.0299 - acc: 0.0125 - val_loss: 0.0186 - val_acc: 0.0000e+00\n",
            "Epoch 243/500\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.0296 - acc: 0.0125 - val_loss: 0.0185 - val_acc: 0.0000e+00\n",
            "Epoch 244/500\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.0294 - acc: 0.0125 - val_loss: 0.0184 - val_acc: 0.0000e+00\n",
            "Epoch 245/500\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.0292 - acc: 0.0125 - val_loss: 0.0183 - val_acc: 0.0000e+00\n",
            "Epoch 246/500\n",
            "80/80 [==============================] - 0s 394us/step - loss: 0.0289 - acc: 0.0125 - val_loss: 0.0182 - val_acc: 0.0000e+00\n",
            "Epoch 247/500\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.0287 - acc: 0.0125 - val_loss: 0.0181 - val_acc: 0.0000e+00\n",
            "Epoch 248/500\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.0285 - acc: 0.0125 - val_loss: 0.0181 - val_acc: 0.0000e+00\n",
            "Epoch 249/500\n",
            "80/80 [==============================] - 0s 242us/step - loss: 0.0283 - acc: 0.0125 - val_loss: 0.0180 - val_acc: 0.0000e+00\n",
            "Epoch 250/500\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.0281 - acc: 0.0125 - val_loss: 0.0179 - val_acc: 0.0000e+00\n",
            "Epoch 251/500\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.0279 - acc: 0.0125 - val_loss: 0.0179 - val_acc: 0.0000e+00\n",
            "Epoch 252/500\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.0276 - acc: 0.0125 - val_loss: 0.0178 - val_acc: 0.0000e+00\n",
            "Epoch 253/500\n",
            "80/80 [==============================] - 0s 318us/step - loss: 0.0276 - acc: 0.0125 - val_loss: 0.0178 - val_acc: 0.0000e+00\n",
            "Epoch 254/500\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.0274 - acc: 0.0125 - val_loss: 0.0177 - val_acc: 0.0000e+00\n",
            "Epoch 255/500\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.0271 - acc: 0.0125 - val_loss: 0.0176 - val_acc: 0.0000e+00\n",
            "Epoch 256/500\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.0270 - acc: 0.0125 - val_loss: 0.0177 - val_acc: 0.0000e+00\n",
            "Epoch 257/500\n",
            "80/80 [==============================] - 0s 336us/step - loss: 0.0268 - acc: 0.0125 - val_loss: 0.0176 - val_acc: 0.0000e+00\n",
            "Epoch 258/500\n",
            "80/80 [==============================] - 0s 333us/step - loss: 0.0267 - acc: 0.0125 - val_loss: 0.0175 - val_acc: 0.0000e+00\n",
            "Epoch 259/500\n",
            "80/80 [==============================] - 0s 318us/step - loss: 0.0264 - acc: 0.0125 - val_loss: 0.0175 - val_acc: 0.0000e+00\n",
            "Epoch 260/500\n",
            "80/80 [==============================] - 0s 326us/step - loss: 0.0264 - acc: 0.0125 - val_loss: 0.0175 - val_acc: 0.0000e+00\n",
            "Epoch 261/500\n",
            "80/80 [==============================] - 0s 345us/step - loss: 0.0265 - acc: 0.0125 - val_loss: 0.0175 - val_acc: 0.0000e+00\n",
            "Epoch 262/500\n",
            "80/80 [==============================] - 0s 327us/step - loss: 0.0261 - acc: 0.0125 - val_loss: 0.0173 - val_acc: 0.0000e+00\n",
            "Epoch 263/500\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.0260 - acc: 0.0125 - val_loss: 0.0174 - val_acc: 0.0000e+00\n",
            "Epoch 264/500\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.0258 - acc: 0.0125 - val_loss: 0.0173 - val_acc: 0.0000e+00\n",
            "Epoch 265/500\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.0256 - acc: 0.0125 - val_loss: 0.0171 - val_acc: 0.0000e+00\n",
            "Epoch 266/500\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.0254 - acc: 0.0125 - val_loss: 0.0172 - val_acc: 0.0000e+00\n",
            "Epoch 267/500\n",
            "80/80 [==============================] - 0s 338us/step - loss: 0.0253 - acc: 0.0125 - val_loss: 0.0173 - val_acc: 0.0000e+00\n",
            "Epoch 268/500\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.0253 - acc: 0.0125 - val_loss: 0.0173 - val_acc: 0.0000e+00\n",
            "Epoch 269/500\n",
            "80/80 [==============================] - 0s 325us/step - loss: 0.0251 - acc: 0.0125 - val_loss: 0.0171 - val_acc: 0.0000e+00\n",
            "Epoch 270/500\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.0249 - acc: 0.0125 - val_loss: 0.0170 - val_acc: 0.0000e+00\n",
            "Epoch 271/500\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.0248 - acc: 0.0125 - val_loss: 0.0169 - val_acc: 0.0000e+00\n",
            "Epoch 272/500\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.0247 - acc: 0.0125 - val_loss: 0.0169 - val_acc: 0.0000e+00\n",
            "Epoch 273/500\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.0246 - acc: 0.0125 - val_loss: 0.0169 - val_acc: 0.0000e+00\n",
            "Epoch 274/500\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.0245 - acc: 0.0125 - val_loss: 0.0169 - val_acc: 0.0000e+00\n",
            "Epoch 275/500\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.0243 - acc: 0.0125 - val_loss: 0.0168 - val_acc: 0.0000e+00\n",
            "Epoch 276/500\n",
            "80/80 [==============================] - 0s 393us/step - loss: 0.0242 - acc: 0.0125 - val_loss: 0.0168 - val_acc: 0.0000e+00\n",
            "Epoch 277/500\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.0242 - acc: 0.0125 - val_loss: 0.0167 - val_acc: 0.0000e+00\n",
            "Epoch 278/500\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.0240 - acc: 0.0125 - val_loss: 0.0167 - val_acc: 0.0000e+00\n",
            "Epoch 279/500\n",
            "80/80 [==============================] - 0s 264us/step - loss: 0.0239 - acc: 0.0125 - val_loss: 0.0167 - val_acc: 0.0000e+00\n",
            "Epoch 280/500\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.0238 - acc: 0.0125 - val_loss: 0.0167 - val_acc: 0.0000e+00\n",
            "Epoch 281/500\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.0237 - acc: 0.0125 - val_loss: 0.0166 - val_acc: 0.0000e+00\n",
            "Epoch 282/500\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.0236 - acc: 0.0125 - val_loss: 0.0166 - val_acc: 0.0000e+00\n",
            "Epoch 283/500\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.0235 - acc: 0.0125 - val_loss: 0.0166 - val_acc: 0.0000e+00\n",
            "Epoch 284/500\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.0234 - acc: 0.0125 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
            "Epoch 285/500\n",
            "80/80 [==============================] - 0s 325us/step - loss: 0.0233 - acc: 0.0125 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
            "Epoch 286/500\n",
            "80/80 [==============================] - 0s 402us/step - loss: 0.0232 - acc: 0.0125 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
            "Epoch 287/500\n",
            "80/80 [==============================] - 0s 440us/step - loss: 0.0231 - acc: 0.0125 - val_loss: 0.0164 - val_acc: 0.0000e+00\n",
            "Epoch 288/500\n",
            "80/80 [==============================] - 0s 399us/step - loss: 0.0230 - acc: 0.0125 - val_loss: 0.0164 - val_acc: 0.0000e+00\n",
            "Epoch 289/500\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.0230 - acc: 0.0125 - val_loss: 0.0164 - val_acc: 0.0000e+00\n",
            "Epoch 290/500\n",
            "80/80 [==============================] - 0s 436us/step - loss: 0.0229 - acc: 0.0125 - val_loss: 0.0163 - val_acc: 0.0000e+00\n",
            "Epoch 291/500\n",
            "80/80 [==============================] - 0s 430us/step - loss: 0.0228 - acc: 0.0125 - val_loss: 0.0163 - val_acc: 0.0000e+00\n",
            "Epoch 292/500\n",
            "80/80 [==============================] - 0s 361us/step - loss: 0.0227 - acc: 0.0125 - val_loss: 0.0163 - val_acc: 0.0000e+00\n",
            "Epoch 293/500\n",
            "80/80 [==============================] - 0s 385us/step - loss: 0.0226 - acc: 0.0125 - val_loss: 0.0163 - val_acc: 0.0000e+00\n",
            "Epoch 294/500\n",
            "80/80 [==============================] - 0s 421us/step - loss: 0.0225 - acc: 0.0125 - val_loss: 0.0162 - val_acc: 0.0000e+00\n",
            "Epoch 295/500\n",
            "80/80 [==============================] - 0s 402us/step - loss: 0.0225 - acc: 0.0125 - val_loss: 0.0162 - val_acc: 0.0000e+00\n",
            "Epoch 296/500\n",
            "80/80 [==============================] - 0s 393us/step - loss: 0.0224 - acc: 0.0125 - val_loss: 0.0161 - val_acc: 0.0000e+00\n",
            "Epoch 297/500\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.0223 - acc: 0.0125 - val_loss: 0.0161 - val_acc: 0.0000e+00\n",
            "Epoch 298/500\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.0222 - acc: 0.0125 - val_loss: 0.0161 - val_acc: 0.0000e+00\n",
            "Epoch 299/500\n",
            "80/80 [==============================] - 0s 237us/step - loss: 0.0221 - acc: 0.0125 - val_loss: 0.0160 - val_acc: 0.0000e+00\n",
            "Epoch 300/500\n",
            "80/80 [==============================] - 0s 235us/step - loss: 0.0221 - acc: 0.0125 - val_loss: 0.0160 - val_acc: 0.0000e+00\n",
            "Epoch 301/500\n",
            "80/80 [==============================] - 0s 267us/step - loss: 0.0220 - acc: 0.0125 - val_loss: 0.0159 - val_acc: 0.0000e+00\n",
            "Epoch 302/500\n",
            "80/80 [==============================] - 0s 241us/step - loss: 0.0219 - acc: 0.0125 - val_loss: 0.0159 - val_acc: 0.0000e+00\n",
            "Epoch 303/500\n",
            "80/80 [==============================] - 0s 257us/step - loss: 0.0219 - acc: 0.0125 - val_loss: 0.0158 - val_acc: 0.0000e+00\n",
            "Epoch 304/500\n",
            "80/80 [==============================] - 0s 224us/step - loss: 0.0219 - acc: 0.0125 - val_loss: 0.0158 - val_acc: 0.0000e+00\n",
            "Epoch 305/500\n",
            "80/80 [==============================] - 0s 255us/step - loss: 0.0217 - acc: 0.0125 - val_loss: 0.0158 - val_acc: 0.0000e+00\n",
            "Epoch 306/500\n",
            "80/80 [==============================] - 0s 255us/step - loss: 0.0217 - acc: 0.0125 - val_loss: 0.0159 - val_acc: 0.0000e+00\n",
            "Epoch 307/500\n",
            "80/80 [==============================] - 0s 228us/step - loss: 0.0218 - acc: 0.0125 - val_loss: 0.0160 - val_acc: 0.0000e+00\n",
            "Epoch 308/500\n",
            "80/80 [==============================] - 0s 250us/step - loss: 0.0217 - acc: 0.0125 - val_loss: 0.0159 - val_acc: 0.0000e+00\n",
            "Epoch 309/500\n",
            "80/80 [==============================] - 0s 233us/step - loss: 0.0215 - acc: 0.0125 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 310/500\n",
            "80/80 [==============================] - 0s 230us/step - loss: 0.0214 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 311/500\n",
            "80/80 [==============================] - 0s 244us/step - loss: 0.0214 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 312/500\n",
            "80/80 [==============================] - 0s 233us/step - loss: 0.0214 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 313/500\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.0213 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 314/500\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.0213 - acc: 0.0125 - val_loss: 0.0158 - val_acc: 0.0000e+00\n",
            "Epoch 315/500\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.0212 - acc: 0.0125 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 316/500\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.0211 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 317/500\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.0211 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 318/500\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.0210 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 319/500\n",
            "80/80 [==============================] - 0s 245us/step - loss: 0.0210 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 320/500\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.0210 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 321/500\n",
            "80/80 [==============================] - 0s 248us/step - loss: 0.0210 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 322/500\n",
            "80/80 [==============================] - 0s 243us/step - loss: 0.0208 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 323/500\n",
            "80/80 [==============================] - 0s 236us/step - loss: 0.0208 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 324/500\n",
            "80/80 [==============================] - 0s 261us/step - loss: 0.0207 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 325/500\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.0207 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 326/500\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.0206 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 327/500\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.0206 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 328/500\n",
            "80/80 [==============================] - 0s 252us/step - loss: 0.0206 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 329/500\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.0205 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 330/500\n",
            "80/80 [==============================] - 0s 255us/step - loss: 0.0205 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 331/500\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.0204 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 332/500\n",
            "80/80 [==============================] - 0s 259us/step - loss: 0.0204 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 333/500\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.0203 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 334/500\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.0203 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 335/500\n",
            "80/80 [==============================] - 0s 234us/step - loss: 0.0203 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 336/500\n",
            "80/80 [==============================] - 0s 235us/step - loss: 0.0202 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 337/500\n",
            "80/80 [==============================] - 0s 271us/step - loss: 0.0202 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 338/500\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.0201 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 339/500\n",
            "80/80 [==============================] - 0s 356us/step - loss: 0.0201 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 340/500\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.0200 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 341/500\n",
            "80/80 [==============================] - 0s 239us/step - loss: 0.0200 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 342/500\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.0200 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 343/500\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.0199 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 344/500\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.0199 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 345/500\n",
            "80/80 [==============================] - 0s 374us/step - loss: 0.0198 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 346/500\n",
            "80/80 [==============================] - 0s 315us/step - loss: 0.0199 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 347/500\n",
            "80/80 [==============================] - 0s 320us/step - loss: 0.0198 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 348/500\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.0199 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 349/500\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.0198 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 350/500\n",
            "80/80 [==============================] - 0s 339us/step - loss: 0.0197 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 351/500\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.0197 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 352/500\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.0197 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 353/500\n",
            "80/80 [==============================] - 0s 382us/step - loss: 0.0197 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 354/500\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.0196 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 355/500\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.0195 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 356/500\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.0196 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 357/500\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.0195 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 358/500\n",
            "80/80 [==============================] - 0s 353us/step - loss: 0.0195 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 359/500\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.0195 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 360/500\n",
            "80/80 [==============================] - 0s 392us/step - loss: 0.0194 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 361/500\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.0194 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 362/500\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.0194 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 363/500\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 364/500\n",
            "80/80 [==============================] - 0s 306us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 365/500\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 366/500\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 367/500\n",
            "80/80 [==============================] - 0s 326us/step - loss: 0.0192 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 368/500\n",
            "80/80 [==============================] - 0s 378us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 369/500\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 370/500\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 371/500\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.0192 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 372/500\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 373/500\n",
            "80/80 [==============================] - 0s 397us/step - loss: 0.0192 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 374/500\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.0192 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 375/500\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 376/500\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 377/500\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 378/500\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 379/500\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 380/500\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 381/500\n",
            "80/80 [==============================] - 0s 379us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 382/500\n",
            "80/80 [==============================] - 0s 344us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 383/500\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 384/500\n",
            "80/80 [==============================] - 0s 332us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 385/500\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 386/500\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 387/500\n",
            "80/80 [==============================] - 0s 326us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 388/500\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 389/500\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 390/500\n",
            "80/80 [==============================] - 0s 363us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 391/500\n",
            "80/80 [==============================] - 0s 357us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 392/500\n",
            "80/80 [==============================] - 0s 457us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 393/500\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 394/500\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 395/500\n",
            "80/80 [==============================] - 0s 357us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 396/500\n",
            "80/80 [==============================] - 0s 365us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 397/500\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 398/500\n",
            "80/80 [==============================] - 0s 343us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 399/500\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 400/500\n",
            "80/80 [==============================] - 0s 432us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 401/500\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 402/500\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 403/500\n",
            "80/80 [==============================] - 0s 405us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 404/500\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 405/500\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 406/500\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 407/500\n",
            "80/80 [==============================] - 0s 354us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 408/500\n",
            "80/80 [==============================] - 0s 347us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 409/500\n",
            "80/80 [==============================] - 0s 404us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 410/500\n",
            "80/80 [==============================] - 0s 347us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 411/500\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.0185 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 412/500\n",
            "80/80 [==============================] - 0s 408us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 413/500\n",
            "80/80 [==============================] - 0s 365us/step - loss: 0.0185 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 414/500\n",
            "80/80 [==============================] - 0s 339us/step - loss: 0.0185 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 415/500\n",
            "80/80 [==============================] - 0s 257us/step - loss: 0.0185 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 416/500\n",
            "80/80 [==============================] - 0s 259us/step - loss: 0.0185 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 417/500\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 418/500\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.0185 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 419/500\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 420/500\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 421/500\n",
            "80/80 [==============================] - 0s 325us/step - loss: 0.0185 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 422/500\n",
            "80/80 [==============================] - 0s 341us/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 423/500\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 424/500\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 425/500\n",
            "80/80 [==============================] - 0s 333us/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 426/500\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 427/500\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 428/500\n",
            "80/80 [==============================] - 0s 337us/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 429/500\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 430/500\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 431/500\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 432/500\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 433/500\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 434/500\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 435/500\n",
            "80/80 [==============================] - 0s 246us/step - loss: 0.0182 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 436/500\n",
            "80/80 [==============================] - 0s 346us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 437/500\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 438/500\n",
            "80/80 [==============================] - 0s 355us/step - loss: 0.0182 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 439/500\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 440/500\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 441/500\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 442/500\n",
            "80/80 [==============================] - 0s 270us/step - loss: 0.0182 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 443/500\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.0182 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 444/500\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 445/500\n",
            "80/80 [==============================] - 0s 247us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 446/500\n",
            "80/80 [==============================] - 0s 271us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 447/500\n",
            "80/80 [==============================] - 0s 265us/step - loss: 0.0182 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 448/500\n",
            "80/80 [==============================] - 0s 268us/step - loss: 0.0182 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 449/500\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 450/500\n",
            "80/80 [==============================] - 0s 257us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 451/500\n",
            "80/80 [==============================] - 0s 267us/step - loss: 0.0182 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 452/500\n",
            "80/80 [==============================] - 0s 257us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 453/500\n",
            "80/80 [==============================] - 0s 252us/step - loss: 0.0182 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 454/500\n",
            "80/80 [==============================] - 0s 382us/step - loss: 0.0182 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 455/500\n",
            "80/80 [==============================] - 0s 320us/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 456/500\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 457/500\n",
            "80/80 [==============================] - 0s 342us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 458/500\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 459/500\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 460/500\n",
            "80/80 [==============================] - 0s 265us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 461/500\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 462/500\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 463/500\n",
            "80/80 [==============================] - 0s 250us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 464/500\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 465/500\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 466/500\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 467/500\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 468/500\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 469/500\n",
            "80/80 [==============================] - 0s 389us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 470/500\n",
            "80/80 [==============================] - 0s 343us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 471/500\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 472/500\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 473/500\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 474/500\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 475/500\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 476/500\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 477/500\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 478/500\n",
            "80/80 [==============================] - 0s 333us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 479/500\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 480/500\n",
            "80/80 [==============================] - 0s 320us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 481/500\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 482/500\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 483/500\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 484/500\n",
            "80/80 [==============================] - 0s 331us/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 485/500\n",
            "80/80 [==============================] - 0s 350us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 486/500\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.0180 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 487/500\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 488/500\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 489/500\n",
            "80/80 [==============================] - 0s 314us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 490/500\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 491/500\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 492/500\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 493/500\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 494/500\n",
            "80/80 [==============================] - 0s 365us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 495/500\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 496/500\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 497/500\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.0178 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 498/500\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 499/500\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 500/500\n",
            "80/80 [==============================] - 0s 425us/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1rgp3ioqvpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1N2kGiarMe4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "27320dd4-a457-40c8-ca72-838eaebc6fef"
      },
      "source": [
        "plt.scatter(range(20),results,c='r')\n",
        "plt.scatter(range(20),y_test,c='g')\n",
        "plt.show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX9PG9o2riOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "feb27519-8a4d-4f8e-ff89-01f296779d2e"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.show()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDwhbBAt6Fkm/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3lREurnsFFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We learned 2 things firstly when we setted epoch to 50 and we didnt normalize our data we did not get good results\n",
        "#but after normalizing and increasing the epoch to 500 our model fitted almost perfectly\n",
        "#We can also add a new layer which will considerably decrease the epochs required for convergence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOS_9hTQszHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}